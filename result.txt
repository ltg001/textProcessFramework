¡¾char 1¡¿
C:\Users\Tabulator\Desktop\pytorch_implementations\Chinese-Text-Classification-Pytorch>python run.py --model TextCNN
Loading data...
Vocab size: 4762
180000it [00:02, 76560.58it/s]
10000it [00:00, 81244.18it/s]
10000it [00:00, 56960.43it/s]
Time usage: 0:00:03
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc: 11.72%,  Val Loss:   2.7,  Val Acc: 10.68%,  Time: 0:00:01 *
Iter:    100,  Train Loss:  0.75,  Train Acc: 71.88%,  Val Loss:   0.7,  Val Acc: 78.15%,  Time: 0:00:07 *
Iter:    200,  Train Loss:  0.69,  Train Acc: 76.56%,  Val Loss:  0.55,  Val Acc: 83.21%,  Time: 0:00:12 *
Iter:    300,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 84.61%,  Time: 0:00:17 *
Iter:    400,  Train Loss:  0.69,  Train Acc: 76.56%,  Val Loss:  0.48,  Val Acc: 85.30%,  Time: 0:00:23 *
Iter:    500,  Train Loss:  0.38,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 86.13%,  Time: 0:00:28 *
Iter:    600,  Train Loss:  0.54,  Train Acc: 82.03%,  Val Loss:  0.43,  Val Acc: 86.49%,  Time: 0:00:33 *
Iter:    700,  Train Loss:  0.43,  Train Acc: 84.38%,  Val Loss:  0.41,  Val Acc: 87.31%,  Time: 0:00:39 *
Iter:    800,  Train Loss:  0.42,  Train Acc: 89.06%,  Val Loss:  0.39,  Val Acc: 87.87%,  Time: 0:00:44 *
Iter:    900,  Train Loss:  0.42,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 87.91%,  Time: 0:00:49 *
Iter:   1000,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.38,  Val Acc: 88.13%,  Time: 0:00:55 *
Iter:   1100,  Train Loss:  0.43,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.69%,  Time: 0:01:00 *
Iter:   1200,  Train Loss:  0.43,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 88.99%,  Time: 0:01:05 *
Iter:   1300,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.68%,  Time: 0:01:11
Iter:   1400,  Train Loss:  0.53,  Train Acc: 84.38%,  Val Loss:  0.36,  Val Acc: 88.83%,  Time: 0:01:16 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.42,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.95%,  Time: 0:01:21 *
Iter:   1600,  Train Loss:  0.32,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.02%,  Time: 0:01:26 *
Iter:   1700,  Train Loss:  0.39,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.41%,  Time: 0:01:32 *
Iter:   1800,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 88.99%,  Time: 0:01:37
Iter:   1900,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 89.29%,  Time: 0:01:42 *
Iter:   2000,  Train Loss:  0.37,  Train Acc: 85.94%,  Val Loss:  0.34,  Val Acc: 89.40%,  Time: 0:01:48 *
Iter:   2100,  Train Loss:  0.44,  Train Acc: 89.06%,  Val Loss:  0.34,  Val Acc: 89.38%,  Time: 0:01:53 *
Iter:   2200,  Train Loss:   0.3,  Train Acc: 88.28%,  Val Loss:  0.34,  Val Acc: 89.72%,  Time: 0:01:58 *
Iter:   2300,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.34,  Val Acc: 89.75%,  Time: 0:02:04 *
Iter:   2400,  Train Loss:  0.27,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.66%,  Time: 0:02:09 *
Iter:   2500,  Train Loss:  0.18,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.82%,  Time: 0:02:14 *
Iter:   2600,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.33,  Val Acc: 89.93%,  Time: 0:02:20 *
Iter:   2700,  Train Loss:   0.3,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 89.75%,  Time: 0:02:25
Iter:   2800,  Train Loss:  0.36,  Train Acc: 84.38%,  Val Loss:  0.34,  Val Acc: 89.48%,  Time: 0:02:30
Epoch [3/20]
Iter:   2900,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.33,  Val Acc: 89.98%,  Time: 0:02:36 *
Iter:   3000,  Train Loss:  0.29,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 89.77%,  Time: 0:02:41
Iter:   3100,  Train Loss:  0.29,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 89.77%,  Time: 0:02:46
Iter:   3200,  Train Loss:  0.41,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 89.81%,  Time: 0:02:51
Iter:   3300,  Train Loss:  0.33,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 90.22%,  Time: 0:02:57 *
Iter:   3400,  Train Loss:  0.28,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 89.99%,  Time: 0:03:02
Iter:   3500,  Train Loss:  0.21,  Train Acc: 94.53%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:03:07
Iter:   3600,  Train Loss:  0.14,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 89.87%,  Time: 0:03:13
Iter:   3700,  Train Loss:   0.3,  Train Acc: 87.50%,  Val Loss:  0.33,  Val Acc: 90.07%,  Time: 0:03:18
Iter:   3800,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.88%,  Time: 0:03:23
Iter:   3900,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:  0.34,  Val Acc: 89.84%,  Time: 0:03:29
Iter:   4000,  Train Loss:  0.22,  Train Acc: 95.31%,  Val Loss:  0.33,  Val Acc: 90.25%,  Time: 0:03:34
Iter:   4100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.33,  Val Acc: 90.10%,  Time: 0:03:39
Iter:   4200,  Train Loss:  0.36,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 90.22%,  Time: 0:03:45 *
Epoch [4/20]
Iter:   4300,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 90.15%,  Time: 0:03:50
Iter:   4400,  Train Loss:  0.14,  Train Acc: 96.09%,  Val Loss:  0.32,  Val Acc: 90.27%,  Time: 0:03:55
Iter:   4500,  Train Loss:  0.34,  Train Acc: 92.97%,  Val Loss:  0.33,  Val Acc: 90.24%,  Time: 0:04:00
Iter:   4600,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.33,  Val Acc: 90.17%,  Time: 0:04:06
Iter:   4700,  Train Loss:  0.32,  Train Acc: 92.19%,  Val Loss:  0.32,  Val Acc: 90.33%,  Time: 0:04:11 *
Iter:   4800,  Train Loss:  0.21,  Train Acc: 89.06%,  Val Loss:  0.32,  Val Acc: 90.42%,  Time: 0:04:16
Iter:   4900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.25%,  Time: 0:04:22
Iter:   5000,  Train Loss:  0.21,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 90.18%,  Time: 0:04:27
Iter:   5100,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.32,  Val Acc: 90.29%,  Time: 0:04:32
Iter:   5200,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.32,  Val Acc: 90.42%,  Time: 0:04:38
Iter:   5300,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 90.25%,  Time: 0:04:43
Iter:   5400,  Train Loss:  0.39,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 90.35%,  Time: 0:04:48
Iter:   5500,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.32,  Val Acc: 90.30%,  Time: 0:04:54
Iter:   5600,  Train Loss:  0.17,  Train Acc: 92.19%,  Val Loss:  0.32,  Val Acc: 90.15%,  Time: 0:04:59
Epoch [5/20]
Iter:   5700,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.32,  Val Acc: 90.47%,  Time: 0:05:04
No optimization for a long time, auto-stopping...
Test Loss:   0.3,  Test Acc: 90.96%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9191    0.8980    0.9084      1000
       realty     0.9216    0.9290    0.9253      1000
       stocks     0.8927    0.8400    0.8655      1000
    education     0.9650    0.9380    0.9513      1000
      science     0.8619    0.8740    0.8679      1000
      society     0.8800    0.9170    0.8981      1000
     politics     0.8728    0.9130    0.8925      1000
       sports     0.9512    0.9560    0.9536      1000
         game     0.9050    0.9240    0.9144      1000
entertainment     0.9312    0.9070    0.9189      1000

  avg / total     0.9101    0.9096    0.9096     10000

Confusion Matrix...
[[898  14  45   1   9  12  10   5   3   3]
 [ 10 929   8   1   7  19  12   4   4   6]
 [ 48  24 840   3  34   3  38   2   7   1]
 [  2   3   3 938   6  19   9   5   3  12]
 [  2   4  21   5 874  20  22   3  36  13]
 [  3  18   1  13   8 917  26   1   5   8]
 [  8   8  15   4  16  25 913   3   3   5]
 [  1   1   2   1   6   9   6 956   3  15]
 [  3   0   5   3  42   5   5   9 924   4]
 [  2   7   1   3  12  13   5  17  33 907]]
Time usage: 0:00:00

¡¾char 2¡¿
C:\Users\Tabulator\Desktop\pytorch_implementations\Chinese-Text-Classification-Pytorch>python run.py --model TextCNN --embedding random
Loading data...
Vocab size: 4762
180000it [00:02, 77391.93it/s]
10000it [00:00, 82843.74it/s]
10000it [00:00, 58406.32it/s]
Time usage: 0:00:03
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300, padding_idx=4761)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.4,  Train Acc:  8.59%,  Val Loss:   2.3,  Val Acc: 21.64%,  Time: 0:00:01 *
Iter:    100,  Train Loss:  0.94,  Train Acc: 67.19%,  Val Loss:  0.69,  Val Acc: 78.81%,  Time: 0:00:07 *
Iter:    200,  Train Loss:  0.93,  Train Acc: 71.09%,  Val Loss:  0.58,  Val Acc: 81.90%,  Time: 0:00:12 *
Iter:    300,  Train Loss:  0.74,  Train Acc: 76.56%,  Val Loss:  0.53,  Val Acc: 83.65%,  Time: 0:00:17 *
Iter:    400,  Train Loss:  0.99,  Train Acc: 71.88%,  Val Loss:  0.53,  Val Acc: 83.55%,  Time: 0:00:23
Iter:    500,  Train Loss:  0.64,  Train Acc: 82.03%,  Val Loss:  0.48,  Val Acc: 85.25%,  Time: 0:00:28 *
Iter:    600,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.48,  Val Acc: 85.13%,  Time: 0:00:33
Iter:    700,  Train Loss:  0.77,  Train Acc: 75.00%,  Val Loss:  0.45,  Val Acc: 85.77%,  Time: 0:00:39 *
Iter:    800,  Train Loss:  0.52,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 85.61%,  Time: 0:00:44
Iter:    900,  Train Loss:  0.55,  Train Acc: 85.94%,  Val Loss:  0.43,  Val Acc: 86.88%,  Time: 0:00:49 *
Iter:   1000,  Train Loss:  0.65,  Train Acc: 80.47%,  Val Loss:  0.44,  Val Acc: 86.26%,  Time: 0:00:55
Iter:   1100,  Train Loss:  0.52,  Train Acc: 88.28%,  Val Loss:  0.43,  Val Acc: 86.55%,  Time: 0:01:00 *
Iter:   1200,  Train Loss:   0.6,  Train Acc: 82.03%,  Val Loss:  0.42,  Val Acc: 87.27%,  Time: 0:01:05 *
Iter:   1300,  Train Loss:  0.56,  Train Acc: 79.69%,  Val Loss:  0.41,  Val Acc: 87.46%,  Time: 0:01:11 *
Iter:   1400,  Train Loss:  0.78,  Train Acc: 75.78%,  Val Loss:  0.41,  Val Acc: 87.32%,  Time: 0:01:16
Epoch [2/20]
Iter:   1500,  Train Loss:   0.5,  Train Acc: 87.50%,  Val Loss:  0.39,  Val Acc: 88.01%,  Time: 0:01:21 *
Iter:   1600,  Train Loss:  0.31,  Train Acc: 91.41%,  Val Loss:   0.4,  Val Acc: 87.99%,  Time: 0:01:27
Iter:   1700,  Train Loss:  0.42,  Train Acc: 88.28%,  Val Loss:  0.39,  Val Acc: 88.13%,  Time: 0:01:32 *
Iter:   1800,  Train Loss:  0.46,  Train Acc: 88.28%,  Val Loss:   0.4,  Val Acc: 87.63%,  Time: 0:01:37
Iter:   1900,  Train Loss:   0.5,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 88.28%,  Time: 0:01:42 *
Iter:   2000,  Train Loss:  0.55,  Train Acc: 82.81%,  Val Loss:  0.39,  Val Acc: 88.38%,  Time: 0:01:48
Iter:   2100,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 88.27%,  Time: 0:01:53
Iter:   2200,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.37,  Val Acc: 88.55%,  Time: 0:01:58 *
Iter:   2300,  Train Loss:  0.42,  Train Acc: 89.06%,  Val Loss:  0.37,  Val Acc: 88.54%,  Time: 0:02:04
Iter:   2400,  Train Loss:  0.37,  Train Acc: 92.19%,  Val Loss:  0.39,  Val Acc: 88.34%,  Time: 0:02:09
Iter:   2500,  Train Loss:  0.25,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 88.91%,  Time: 0:02:14 *
Iter:   2600,  Train Loss:  0.42,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 88.96%,  Time: 0:02:19
Iter:   2700,  Train Loss:  0.37,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 88.49%,  Time: 0:02:25
Iter:   2800,  Train Loss:  0.54,  Train Acc: 81.25%,  Val Loss:  0.36,  Val Acc: 88.63%,  Time: 0:02:30
Epoch [3/20]
Iter:   2900,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.22%,  Time: 0:02:35 *
Iter:   3000,  Train Loss:  0.47,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 88.95%,  Time: 0:02:41
Iter:   3100,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.37,  Val Acc: 88.49%,  Time: 0:02:46
Iter:   3200,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 88.49%,  Time: 0:02:51
Iter:   3300,  Train Loss:  0.39,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 89.17%,  Time: 0:02:57 *
Iter:   3400,  Train Loss:  0.42,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 89.06%,  Time: 0:03:02
Iter:   3500,  Train Loss:  0.27,  Train Acc: 89.84%,  Val Loss:  0.35,  Val Acc: 89.38%,  Time: 0:03:07 *
Iter:   3600,  Train Loss:  0.19,  Train Acc: 93.75%,  Val Loss:  0.36,  Val Acc: 89.18%,  Time: 0:03:12
Iter:   3700,  Train Loss:  0.36,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 89.17%,  Time: 0:03:18
Iter:   3800,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 88.95%,  Time: 0:03:23
Iter:   3900,  Train Loss:  0.37,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 88.66%,  Time: 0:03:28
Iter:   4000,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.35,  Val Acc: 89.56%,  Time: 0:03:34
Iter:   4100,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.26%,  Time: 0:03:39
Iter:   4200,  Train Loss:  0.45,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.24%,  Time: 0:03:44
Epoch [4/20]
Iter:   4300,  Train Loss:  0.31,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 89.17%,  Time: 0:03:50
Iter:   4400,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.35,  Val Acc: 89.52%,  Time: 0:03:55 *
Iter:   4500,  Train Loss:  0.54,  Train Acc: 85.16%,  Val Loss:  0.35,  Val Acc: 89.58%,  Time: 0:04:00
Iter:   4600,  Train Loss:  0.39,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.57%,  Time: 0:04:06
Iter:   4700,  Train Loss:  0.39,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 89.71%,  Time: 0:04:11
Iter:   4800,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 89.50%,  Time: 0:04:16
Iter:   4900,  Train Loss:   0.3,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.53%,  Time: 0:04:21
Iter:   5000,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 89.28%,  Time: 0:04:27
Iter:   5100,  Train Loss:  0.37,  Train Acc: 91.41%,  Val Loss:  0.36,  Val Acc: 89.62%,  Time: 0:04:32
Iter:   5200,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 89.86%,  Time: 0:04:37
Iter:   5300,  Train Loss:  0.45,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.79%,  Time: 0:04:43
Iter:   5400,  Train Loss:   0.6,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 89.27%,  Time: 0:04:48
No optimization for a long time, auto-stopping...
Test Loss:  0.34,  Test Acc: 89.75%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9314    0.8550    0.8916      1000
       realty     0.9101    0.9210    0.9155      1000
       stocks     0.8240    0.8520    0.8378      1000
    education     0.9559    0.9530    0.9544      1000
      science     0.8621    0.8500    0.8560      1000
      society     0.8786    0.8970    0.8877      1000
     politics     0.8974    0.8750    0.8861      1000
       sports     0.9097    0.9670    0.9375      1000
         game     0.9240    0.9000    0.9119      1000
entertainment     0.8873    0.9050    0.8960      1000

  avg / total     0.8980    0.8975    0.8974     10000

Confusion Matrix...
[[855  15  80   2   9  17   5  11   4   2]
 [  9 921  22   0   2  16   7   5   2  16]
 [ 36  23 852   2  39   4  29   6   5   4]
 [  1   2   3 953   5   9   8   6   2  11]
 [  3   6  32   5 850  15  18  14  40  17]
 [  3  22   2  18   8 897  25   2   2  21]
 [  8   9  30   8  13  39 875   8   1   9]
 [  0   4   1   1   2   7   2 967   1  15]
 [  1   2   9   2  43   5   2  16 900  20]
 [  2   8   3   6  15  12   4  28  17 905]]
Time usage: 0:00:00

¡¾word 1¡¿
C:\Users\Tabulator\Desktop\pytorch_implementations\Chinese-Text-Classification-Pytorch>python run.py --model TextCNN --word True
Loading data...
0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\TABULA~1\AppData\Local\Temp\jieba.cache
Loading model cost 0.655 seconds.
Prefix dict has been built succesfully.
180000it [00:17, 10267.07it/s]
Vocab size: 8074
180000it [00:18, 9822.05it/s]
10000it [00:01, 9546.63it/s]
10000it [00:00, 10155.86it/s]
Time usage: 0:00:38
<bound method Module.parameters of Model(
  (embedding): Embedding(8074, 300, padding_idx=8073)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc:  7.03%,  Val Loss:   2.8,  Val Acc: 13.98%,  Time: 0:00:01 *
Iter:    100,  Train Loss:   1.3,  Train Acc: 56.25%,  Val Loss:   1.1,  Val Acc: 64.85%,  Time: 0:00:07 *
Iter:    200,  Train Loss:   1.1,  Train Acc: 67.19%,  Val Loss:  0.92,  Val Acc: 70.73%,  Time: 0:00:12 *
Iter:    300,  Train Loss:  0.77,  Train Acc: 76.56%,  Val Loss:  0.82,  Val Acc: 74.15%,  Time: 0:00:17 *
Iter:    400,  Train Loss:   1.2,  Train Acc: 64.84%,  Val Loss:  0.76,  Val Acc: 76.27%,  Time: 0:00:22 *
Iter:    500,  Train Loss:  0.74,  Train Acc: 75.00%,  Val Loss:  0.69,  Val Acc: 78.06%,  Time: 0:00:27 *
Iter:    600,  Train Loss:  0.86,  Train Acc: 75.78%,  Val Loss:  0.66,  Val Acc: 79.15%,  Time: 0:00:32 *
Iter:    700,  Train Loss:  0.75,  Train Acc: 71.88%,  Val Loss:  0.64,  Val Acc: 79.71%,  Time: 0:00:38 *
Iter:    800,  Train Loss:  0.61,  Train Acc: 78.91%,  Val Loss:  0.61,  Val Acc: 80.40%,  Time: 0:00:43 *
Iter:    900,  Train Loss:  0.65,  Train Acc: 79.69%,  Val Loss:  0.58,  Val Acc: 81.77%,  Time: 0:00:48 *
Iter:   1000,  Train Loss:  0.59,  Train Acc: 82.81%,  Val Loss:  0.58,  Val Acc: 81.72%,  Time: 0:00:53
Iter:   1100,  Train Loss:  0.55,  Train Acc: 84.38%,  Val Loss:  0.55,  Val Acc: 82.74%,  Time: 0:00:58 *
Iter:   1200,  Train Loss:  0.66,  Train Acc: 80.47%,  Val Loss:  0.54,  Val Acc: 83.22%,  Time: 0:01:04 *
Iter:   1300,  Train Loss:  0.74,  Train Acc: 77.34%,  Val Loss:  0.53,  Val Acc: 83.10%,  Time: 0:01:09 *
Iter:   1400,  Train Loss:  0.65,  Train Acc: 79.69%,  Val Loss:  0.52,  Val Acc: 83.81%,  Time: 0:01:14 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.57,  Train Acc: 85.16%,  Val Loss:   0.5,  Val Acc: 84.22%,  Time: 0:01:19 *
Iter:   1600,  Train Loss:   0.5,  Train Acc: 85.16%,  Val Loss:   0.5,  Val Acc: 84.14%,  Time: 0:01:24 *
Iter:   1700,  Train Loss:  0.33,  Train Acc: 90.62%,  Val Loss:   0.5,  Val Acc: 84.40%,  Time: 0:01:30 *
Iter:   1800,  Train Loss:  0.52,  Train Acc: 86.72%,  Val Loss:   0.5,  Val Acc: 84.37%,  Time: 0:01:35
Iter:   1900,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 85.03%,  Time: 0:01:40 *
Iter:   2000,  Train Loss:  0.55,  Train Acc: 82.81%,  Val Loss:  0.48,  Val Acc: 84.88%,  Time: 0:01:45 *
Iter:   2100,  Train Loss:  0.58,  Train Acc: 77.34%,  Val Loss:  0.47,  Val Acc: 85.07%,  Time: 0:01:50 *
Iter:   2200,  Train Loss:  0.53,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 84.89%,  Time: 0:01:56
Iter:   2300,  Train Loss:  0.43,  Train Acc: 89.84%,  Val Loss:  0.47,  Val Acc: 85.15%,  Time: 0:02:01 *
Iter:   2400,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.47,  Val Acc: 85.29%,  Time: 0:02:06 *
Iter:   2500,  Train Loss:  0.31,  Train Acc: 87.50%,  Val Loss:  0.47,  Val Acc: 85.38%,  Time: 0:02:11 *
Iter:   2600,  Train Loss:  0.48,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 86.20%,  Time: 0:02:16 *
Iter:   2700,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.47,  Val Acc: 85.64%,  Time: 0:02:22
Iter:   2800,  Train Loss:  0.52,  Train Acc: 80.47%,  Val Loss:  0.46,  Val Acc: 85.84%,  Time: 0:02:27
Epoch [3/20]
Iter:   2900,  Train Loss:  0.38,  Train Acc: 89.06%,  Val Loss:  0.45,  Val Acc: 86.19%,  Time: 0:02:32 *
Iter:   3000,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 86.13%,  Time: 0:02:37 *
Iter:   3100,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.46,  Val Acc: 85.85%,  Time: 0:02:42
Iter:   3200,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 86.24%,  Time: 0:02:48 *
Iter:   3300,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 86.05%,  Time: 0:02:53
Iter:   3400,  Train Loss:  0.56,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 86.05%,  Time: 0:02:58
Iter:   3500,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.46,  Val Acc: 86.23%,  Time: 0:03:03
Iter:   3600,  Train Loss:  0.27,  Train Acc: 91.41%,  Val Loss:  0.45,  Val Acc: 86.39%,  Time: 0:03:08
Iter:   3700,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.46,  Val Acc: 86.37%,  Time: 0:03:14
Iter:   3800,  Train Loss:  0.26,  Train Acc: 89.84%,  Val Loss:  0.46,  Val Acc: 86.11%,  Time: 0:03:19
Iter:   3900,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 85.68%,  Time: 0:03:24
Iter:   4000,  Train Loss:  0.34,  Train Acc: 90.62%,  Val Loss:  0.45,  Val Acc: 86.30%,  Time: 0:03:29
Iter:   4100,  Train Loss:  0.41,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 86.82%,  Time: 0:03:34 *
Iter:   4200,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 86.12%,  Time: 0:03:39
Epoch [4/20]
Iter:   4300,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 86.34%,  Time: 0:03:45
Iter:   4400,  Train Loss:  0.27,  Train Acc: 92.97%,  Val Loss:  0.44,  Val Acc: 86.68%,  Time: 0:03:50 *
Iter:   4500,  Train Loss:   0.5,  Train Acc: 83.59%,  Val Loss:  0.46,  Val Acc: 86.36%,  Time: 0:03:55
Iter:   4600,  Train Loss:  0.25,  Train Acc: 92.19%,  Val Loss:  0.46,  Val Acc: 86.61%,  Time: 0:04:00
Iter:   4700,  Train Loss:  0.47,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 86.46%,  Time: 0:04:05
Iter:   4800,  Train Loss:  0.27,  Train Acc: 89.06%,  Val Loss:  0.46,  Val Acc: 86.84%,  Time: 0:04:11
Iter:   4900,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.46,  Val Acc: 86.68%,  Time: 0:04:16
Iter:   5000,  Train Loss:  0.16,  Train Acc: 95.31%,  Val Loss:  0.46,  Val Acc: 86.44%,  Time: 0:04:21
Iter:   5100,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.47,  Val Acc: 86.70%,  Time: 0:04:26
Iter:   5200,  Train Loss:  0.37,  Train Acc: 87.50%,  Val Loss:  0.47,  Val Acc: 86.86%,  Time: 0:04:31
Iter:   5300,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 86.52%,  Time: 0:04:37
Iter:   5400,  Train Loss:  0.56,  Train Acc: 89.84%,  Val Loss:  0.47,  Val Acc: 86.45%,  Time: 0:04:42
No optimization for a long time, auto-stopping...
Test Loss:   0.4,  Test Acc: 87.65%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9099    0.8480    0.8778      1000
       realty     0.8943    0.9050    0.8996      1000
       stocks     0.7720    0.8600    0.8136      1000
    education     0.9529    0.9310    0.9418      1000
      science     0.8261    0.8360    0.8310      1000
      society     0.8877    0.8380    0.8621      1000
     politics     0.8582    0.8410    0.8495      1000
       sports     0.9239    0.9350    0.9294      1000
         game     0.9241    0.8760    0.8994      1000
entertainment     0.8372    0.8950    0.8652      1000

  avg / total     0.8786    0.8765    0.8770     10000

Confusion Matrix...
[[848  14  89   3  11   7   9   3   4  12]
 [  9 905  39   1   8   7   9   2   2  18]
 [ 48  23 860   1  27   2  26   5   4   4]
 [  2   6   8 931   6  14   9   6   5  13]
 [  5   8  51   6 836  11  28   5  27  23]
 [  7  23   9  18  19 838  39   4   6  37]
 [  6  13  42  11  27  40 841   4   1  15]
 [  1   7   3   1  11   2   4 935   6  30]
 [  2   5   6   2  54   5   5  23 876  22]
 [  4   8   7   3  13  18  10  25  17 895]]
Time usage: 0:00:00

¡¾word 2¡¿
C:\Users\Tabulator\Desktop\pytorch_implementations\Chinese-Text-Classification-Pytorch>python run.py --model TextCNN --word True (change min_freq to 40)
Loading data...
0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\TABULA~1\AppData\Local\Temp\jieba.cache
Loading model cost 0.662 seconds.
Prefix dict has been built succesfully.
180000it [00:17, 10303.65it/s]
Vocab size: 6384
180000it [00:18, 9918.14it/s]
10000it [00:01, 9660.21it/s]
10000it [00:00, 10056.53it/s]
Time usage: 0:00:38
<bound method Module.parameters of Model(
  (embedding): Embedding(6384, 300, padding_idx=6383)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.4,  Train Acc:  8.59%,  Val Loss:   2.8,  Val Acc: 14.54%,  Time: 0:00:01 *
Iter:    100,  Train Loss:   1.3,  Train Acc: 56.25%,  Val Loss:   1.1,  Val Acc: 63.84%,  Time: 0:00:07 *
Iter:    200,  Train Loss:   1.1,  Train Acc: 64.06%,  Val Loss:  0.92,  Val Acc: 70.63%,  Time: 0:00:12 *
Iter:    300,  Train Loss:  0.78,  Train Acc: 75.00%,  Val Loss:  0.82,  Val Acc: 73.86%,  Time: 0:00:17 *
Iter:    400,  Train Loss:   1.1,  Train Acc: 67.97%,  Val Loss:  0.75,  Val Acc: 76.20%,  Time: 0:00:22 *
Iter:    500,  Train Loss:  0.86,  Train Acc: 71.09%,  Val Loss:  0.69,  Val Acc: 77.95%,  Time: 0:00:27 *
Iter:    600,  Train Loss:  0.83,  Train Acc: 71.88%,  Val Loss:  0.67,  Val Acc: 78.70%,  Time: 0:00:32 *
Iter:    700,  Train Loss:  0.82,  Train Acc: 72.66%,  Val Loss:  0.64,  Val Acc: 80.28%,  Time: 0:00:37 *
Iter:    800,  Train Loss:  0.67,  Train Acc: 78.91%,  Val Loss:  0.61,  Val Acc: 80.63%,  Time: 0:00:42 *
Iter:    900,  Train Loss:  0.64,  Train Acc: 78.91%,  Val Loss:  0.59,  Val Acc: 81.38%,  Time: 0:00:48 *
Iter:   1000,  Train Loss:  0.58,  Train Acc: 80.47%,  Val Loss:  0.58,  Val Acc: 81.88%,  Time: 0:00:53 *
Iter:   1100,  Train Loss:   0.6,  Train Acc: 80.47%,  Val Loss:  0.55,  Val Acc: 82.77%,  Time: 0:00:58 *
Iter:   1200,  Train Loss:   0.7,  Train Acc: 80.47%,  Val Loss:  0.56,  Val Acc: 82.72%,  Time: 0:01:03
Iter:   1300,  Train Loss:  0.69,  Train Acc: 76.56%,  Val Loss:  0.55,  Val Acc: 82.69%,  Time: 0:01:08 *
Iter:   1400,  Train Loss:  0.65,  Train Acc: 78.12%,  Val Loss:  0.53,  Val Acc: 83.59%,  Time: 0:01:13 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.62,  Train Acc: 77.34%,  Val Loss:  0.51,  Val Acc: 84.14%,  Time: 0:01:19 *
Iter:   1600,  Train Loss:   0.5,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 84.11%,  Time: 0:01:24
Iter:   1700,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.51,  Val Acc: 84.10%,  Time: 0:01:29 *
Iter:   1800,  Train Loss:  0.44,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 84.13%,  Time: 0:01:34 *
Iter:   1900,  Train Loss:  0.45,  Train Acc: 86.72%,  Val Loss:   0.5,  Val Acc: 84.23%,  Time: 0:01:39 *
Iter:   2000,  Train Loss:  0.54,  Train Acc: 79.69%,  Val Loss:  0.49,  Val Acc: 84.51%,  Time: 0:01:44 *
Iter:   2100,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.49,  Val Acc: 84.45%,  Time: 0:01:50
Iter:   2200,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 84.40%,  Time: 0:01:55
Iter:   2300,  Train Loss:  0.42,  Train Acc: 89.06%,  Val Loss:  0.49,  Val Acc: 84.83%,  Time: 0:02:00 *
Iter:   2400,  Train Loss:  0.39,  Train Acc: 89.06%,  Val Loss:  0.49,  Val Acc: 84.69%,  Time: 0:02:05
Iter:   2500,  Train Loss:  0.29,  Train Acc: 91.41%,  Val Loss:  0.49,  Val Acc: 85.23%,  Time: 0:02:10
Iter:   2600,  Train Loss:  0.47,  Train Acc: 82.03%,  Val Loss:  0.47,  Val Acc: 85.39%,  Time: 0:02:15 *
Iter:   2700,  Train Loss:  0.28,  Train Acc: 91.41%,  Val Loss:  0.48,  Val Acc: 85.22%,  Time: 0:02:20
Iter:   2800,  Train Loss:  0.52,  Train Acc: 81.25%,  Val Loss:  0.47,  Val Acc: 85.54%,  Time: 0:02:26 *
Epoch [3/20]
Iter:   2900,  Train Loss:   0.4,  Train Acc: 87.50%,  Val Loss:  0.47,  Val Acc: 85.87%,  Time: 0:02:31 *
Iter:   3000,  Train Loss:  0.33,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 85.63%,  Time: 0:02:36
Iter:   3100,  Train Loss:  0.38,  Train Acc: 84.38%,  Val Loss:  0.49,  Val Acc: 84.79%,  Time: 0:02:41
Iter:   3200,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 85.75%,  Time: 0:02:46
Iter:   3300,  Train Loss:  0.42,  Train Acc: 85.94%,  Val Loss:  0.48,  Val Acc: 85.03%,  Time: 0:02:51
Iter:   3400,  Train Loss:  0.54,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 85.65%,  Time: 0:02:57
Iter:   3500,  Train Loss:  0.22,  Train Acc: 92.97%,  Val Loss:  0.48,  Val Acc: 85.71%,  Time: 0:03:02
Iter:   3600,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.48,  Val Acc: 85.47%,  Time: 0:03:07
Iter:   3700,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.48,  Val Acc: 85.56%,  Time: 0:03:12
Iter:   3800,  Train Loss:  0.36,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 85.94%,  Time: 0:03:17
Iter:   3900,  Train Loss:  0.52,  Train Acc: 82.03%,  Val Loss:  0.49,  Val Acc: 85.38%,  Time: 0:03:22
No optimization for a long time, auto-stopping...
Test Loss:  0.42,  Test Acc: 86.65%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9005    0.8510    0.8751      1000
       realty     0.9192    0.8870    0.9028      1000
       stocks     0.7870    0.8130    0.7998      1000
    education     0.9418    0.9230    0.9323      1000
      science     0.8148    0.8140    0.8144      1000
      society     0.8205    0.8820    0.8501      1000
     politics     0.8451    0.8350    0.8400      1000
       sports     0.8694    0.9390    0.9029      1000
         game     0.9273    0.8550    0.8897      1000
entertainment     0.8549    0.8660    0.8604      1000

  avg / total     0.8681    0.8665    0.8668     10000

Confusion Matrix...
[[851  13  73   1  13  15  10  10   4  10]
 [ 15 887  31   5   9  18  10   4   3  18]
 [ 59  26 813   1  44   5  39   6   2   5]
 [  0   5   5 923   6  32   8   8   2  11]
 [  5   5  54  10 814  23  30  11  27  21]
 [  5  10   5  14  13 882  36  10   2  23]
 [  5   8  39  16  17  55 835  11   1  13]
 [  0   5   3   3   7   7   6 939   5  25]
 [  2   2   6   1  62   9   7  35 855  21]
 [  3   4   4   6  14  29   7  46  21 866]]
Time usage: 0:00:00

¡¾word 3¡¿
C:\Users\Tabulator\Desktop\pytorch_implementations\Chinese-Text-Classification-Pytorch>python run.py --model TextCNN --word True (change min_freq to 20 & vocab limited to 10000)
Loading data...
0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\TABULA~1\AppData\Local\Temp\jieba.cache
Loading model cost 0.654 seconds.
Prefix dict has been built succesfully.
180000it [00:17, 10465.39it/s]
Vocab size: 10002
180000it [00:18, 9952.89it/s]
10000it [00:01, 9636.22it/s]
10000it [00:00, 10158.15it/s]
Time usage: 0:00:37
<bound method Module.parameters of Model(
  (embedding): Embedding(10002, 300, padding_idx=10001)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc:  9.38%,  Val Loss:   2.7,  Val Acc: 14.91%,  Time: 0:00:01 *
Iter:    100,  Train Loss:   1.3,  Train Acc: 55.47%,  Val Loss:   1.1,  Val Acc: 64.13%,  Time: 0:00:07 *
Iter:    200,  Train Loss:   1.1,  Train Acc: 62.50%,  Val Loss:  0.92,  Val Acc: 70.76%,  Time: 0:00:12 *
Iter:    300,  Train Loss:  0.82,  Train Acc: 72.66%,  Val Loss:  0.83,  Val Acc: 73.71%,  Time: 0:00:17 *
Iter:    400,  Train Loss:   1.1,  Train Acc: 67.19%,  Val Loss:  0.75,  Val Acc: 76.42%,  Time: 0:00:22 *
Iter:    500,  Train Loss:  0.82,  Train Acc: 71.88%,  Val Loss:  0.69,  Val Acc: 78.15%,  Time: 0:00:27 *
Iter:    600,  Train Loss:  0.84,  Train Acc: 73.44%,  Val Loss:  0.67,  Val Acc: 78.89%,  Time: 0:00:33 *
Iter:    700,  Train Loss:  0.85,  Train Acc: 68.75%,  Val Loss:  0.63,  Val Acc: 80.09%,  Time: 0:00:38 *
Iter:    800,  Train Loss:  0.68,  Train Acc: 75.78%,  Val Loss:  0.61,  Val Acc: 80.96%,  Time: 0:00:43 *
Iter:    900,  Train Loss:  0.65,  Train Acc: 83.59%,  Val Loss:  0.58,  Val Acc: 81.98%,  Time: 0:00:48 *
Iter:   1000,  Train Loss:  0.57,  Train Acc: 80.47%,  Val Loss:  0.58,  Val Acc: 81.95%,  Time: 0:00:54
Iter:   1100,  Train Loss:  0.54,  Train Acc: 84.38%,  Val Loss:  0.53,  Val Acc: 83.26%,  Time: 0:00:59 *
Iter:   1200,  Train Loss:  0.64,  Train Acc: 82.81%,  Val Loss:  0.54,  Val Acc: 83.35%,  Time: 0:01:04
Iter:   1300,  Train Loss:  0.61,  Train Acc: 82.81%,  Val Loss:  0.52,  Val Acc: 83.98%,  Time: 0:01:09 *
Iter:   1400,  Train Loss:  0.63,  Train Acc: 81.25%,  Val Loss:  0.53,  Val Acc: 83.37%,  Time: 0:01:14
Epoch [2/20]
Iter:   1500,  Train Loss:  0.53,  Train Acc: 82.81%,  Val Loss:  0.51,  Val Acc: 84.19%,  Time: 0:01:20 *
Iter:   1600,  Train Loss:  0.45,  Train Acc: 86.72%,  Val Loss:   0.5,  Val Acc: 84.55%,  Time: 0:01:25 *
Iter:   1700,  Train Loss:  0.36,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 84.79%,  Time: 0:01:30 *
Iter:   1800,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 84.82%,  Time: 0:01:35 *
Iter:   1900,  Train Loss:  0.45,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 85.50%,  Time: 0:01:41 *
Iter:   2000,  Train Loss:  0.52,  Train Acc: 81.25%,  Val Loss:  0.48,  Val Acc: 85.52%,  Time: 0:01:46
Iter:   2100,  Train Loss:  0.54,  Train Acc: 80.47%,  Val Loss:  0.48,  Val Acc: 85.37%,  Time: 0:01:51 *
Iter:   2200,  Train Loss:  0.42,  Train Acc: 89.06%,  Val Loss:  0.48,  Val Acc: 85.23%,  Time: 0:01:56
Iter:   2300,  Train Loss:  0.48,  Train Acc: 88.28%,  Val Loss:  0.47,  Val Acc: 85.88%,  Time: 0:02:02 *
Iter:   2400,  Train Loss:  0.29,  Train Acc: 92.19%,  Val Loss:  0.47,  Val Acc: 85.75%,  Time: 0:02:07 *
Iter:   2500,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.47,  Val Acc: 86.05%,  Time: 0:02:12 *
Iter:   2600,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 86.10%,  Time: 0:02:17 *
Iter:   2700,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 86.23%,  Time: 0:02:23
Iter:   2800,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 86.10%,  Time: 0:02:28
Epoch [3/20]
Iter:   2900,  Train Loss:  0.36,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 86.79%,  Time: 0:02:33 *
Iter:   3000,  Train Loss:  0.35,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 86.75%,  Time: 0:02:38 *
Iter:   3100,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:  0.46,  Val Acc: 86.13%,  Time: 0:02:43
Iter:   3200,  Train Loss:  0.47,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 86.86%,  Time: 0:02:49
Iter:   3300,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 86.72%,  Time: 0:02:54
Iter:   3400,  Train Loss:  0.49,  Train Acc: 86.72%,  Val Loss:  0.45,  Val Acc: 86.84%,  Time: 0:02:59
Iter:   3500,  Train Loss:  0.26,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 86.47%,  Time: 0:03:04
Iter:   3600,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 86.82%,  Time: 0:03:09
Iter:   3700,  Train Loss:  0.31,  Train Acc: 90.62%,  Val Loss:  0.44,  Val Acc: 87.02%,  Time: 0:03:15
Iter:   3800,  Train Loss:  0.25,  Train Acc: 90.62%,  Val Loss:  0.45,  Val Acc: 86.65%,  Time: 0:03:20
Iter:   3900,  Train Loss:   0.5,  Train Acc: 84.38%,  Val Loss:  0.46,  Val Acc: 86.36%,  Time: 0:03:25
Iter:   4000,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 86.75%,  Time: 0:03:30
No optimization for a long time, auto-stopping...
Test Loss:   0.4,  Test Acc: 87.91%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9085    0.8440    0.8751      1000
       realty     0.9083    0.9110    0.9096      1000
       stocks     0.8037    0.8350    0.8190      1000
    education     0.9278    0.9380    0.9329      1000
      science     0.8459    0.8400    0.8430      1000
      society     0.8595    0.8750    0.8672      1000
     politics     0.8343    0.8710    0.8523      1000
       sports     0.9057    0.9320    0.9187      1000
         game     0.9350    0.8780    0.9056      1000
entertainment     0.8714    0.8670    0.8692      1000

  avg / total     0.8800    0.8791    0.8792     10000

Confusion Matrix...
[[844  18  85   8   8  12   7   5   5   8]
 [ 12 911  22   1   6  11  16   2   3  16]
 [ 47  24 835   2  39   5  36   7   3   2]
 [  0   6   7 938   6  15  11   7   2   8]
 [  4   6  41  13 840  14  32   9  25  16]
 [  5  11   5  18  16 875  36   6   3  25]
 [  4   7  31  15  12  44 871   3   3  10]
 [  4   8   2   4   5   9   9 932   2  25]
 [  2   4   6   4  48   7  12  21 878  18]
 [  7   8   5   8  13  26  14  37  15 867]]
Time usage: 0:00:00

¡¾word 4¡¿
C:\Users\Tabulator\Desktop\pytorch_implementations\Chinese-Text-Classification-Pytorch>python run.py --model TextCNN --word True (change min_freq to 20 & vocab limited to 20000)
Loading data...
0it [00:00, ?it/s]Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\TABULA~1\AppData\Local\Temp\jieba.cache
Loading model cost 0.655 seconds.
Prefix dict has been built succesfully.
180000it [00:17, 10511.65it/s]
Vocab size: 11216
180000it [00:18, 9994.83it/s]
10000it [00:01, 9608.96it/s]
10000it [00:00, 10188.28it/s]
Time usage: 0:00:37
<bound method Module.parameters of Model(
  (embedding): Embedding(11216, 300, padding_idx=11215)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc: 10.16%,  Val Loss:   2.8,  Val Acc: 14.58%,  Time: 0:00:01 *
Iter:    100,  Train Loss:   1.4,  Train Acc: 56.25%,  Val Loss:   1.1,  Val Acc: 64.34%,  Time: 0:00:07 *
Iter:    200,  Train Loss:   1.1,  Train Acc: 62.50%,  Val Loss:  0.91,  Val Acc: 70.91%,  Time: 0:00:12 *
Iter:    300,  Train Loss:  0.88,  Train Acc: 69.53%,  Val Loss:  0.82,  Val Acc: 74.09%,  Time: 0:00:17 *
Iter:    400,  Train Loss:   1.1,  Train Acc: 66.41%,  Val Loss:  0.75,  Val Acc: 76.67%,  Time: 0:00:22 *
Iter:    500,  Train Loss:  0.77,  Train Acc: 73.44%,  Val Loss:   0.7,  Val Acc: 78.29%,  Time: 0:00:28 *
Iter:    600,  Train Loss:  0.85,  Train Acc: 71.88%,  Val Loss:  0.66,  Val Acc: 79.19%,  Time: 0:00:33 *
Iter:    700,  Train Loss:  0.86,  Train Acc: 66.41%,  Val Loss:  0.63,  Val Acc: 80.09%,  Time: 0:00:38 *
Iter:    800,  Train Loss:  0.64,  Train Acc: 77.34%,  Val Loss:   0.6,  Val Acc: 80.99%,  Time: 0:00:43 *
Iter:    900,  Train Loss:  0.71,  Train Acc: 78.91%,  Val Loss:  0.58,  Val Acc: 81.82%,  Time: 0:00:49 *
Iter:   1000,  Train Loss:  0.56,  Train Acc: 82.03%,  Val Loss:  0.58,  Val Acc: 81.99%,  Time: 0:00:54
Iter:   1100,  Train Loss:  0.57,  Train Acc: 81.25%,  Val Loss:  0.54,  Val Acc: 83.12%,  Time: 0:00:59 *
Iter:   1200,  Train Loss:  0.72,  Train Acc: 82.03%,  Val Loss:  0.54,  Val Acc: 83.04%,  Time: 0:01:04
Iter:   1300,  Train Loss:  0.65,  Train Acc: 76.56%,  Val Loss:  0.52,  Val Acc: 83.59%,  Time: 0:01:10 *
Iter:   1400,  Train Loss:  0.62,  Train Acc: 83.59%,  Val Loss:  0.51,  Val Acc: 84.37%,  Time: 0:01:15 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.53,  Train Acc: 81.25%,  Val Loss:  0.49,  Val Acc: 84.67%,  Time: 0:01:20 *
Iter:   1600,  Train Loss:  0.45,  Train Acc: 86.72%,  Val Loss:   0.5,  Val Acc: 84.84%,  Time: 0:01:25
Iter:   1700,  Train Loss:  0.38,  Train Acc: 89.06%,  Val Loss:  0.49,  Val Acc: 84.79%,  Time: 0:01:31 *
Iter:   1800,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.48,  Val Acc: 85.14%,  Time: 0:01:36 *
Iter:   1900,  Train Loss:  0.45,  Train Acc: 89.84%,  Val Loss:  0.47,  Val Acc: 85.68%,  Time: 0:01:41 *
Iter:   2000,  Train Loss:  0.55,  Train Acc: 82.81%,  Val Loss:  0.47,  Val Acc: 85.49%,  Time: 0:01:46 *
Iter:   2100,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.47,  Val Acc: 85.29%,  Time: 0:01:52
Iter:   2200,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.46,  Val Acc: 85.80%,  Time: 0:01:57 *
Iter:   2300,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 85.67%,  Time: 0:02:02
Iter:   2400,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.46,  Val Acc: 86.12%,  Time: 0:02:07 *
Iter:   2500,  Train Loss:  0.29,  Train Acc: 90.62%,  Val Loss:  0.46,  Val Acc: 85.90%,  Time: 0:02:13
Iter:   2600,  Train Loss:  0.44,  Train Acc: 88.28%,  Val Loss:  0.45,  Val Acc: 86.27%,  Time: 0:02:18 *
Iter:   2700,  Train Loss:  0.29,  Train Acc: 89.06%,  Val Loss:  0.45,  Val Acc: 86.48%,  Time: 0:02:23
Iter:   2800,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 86.51%,  Time: 0:02:28
Epoch [3/20]
Iter:   2900,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.44,  Val Acc: 86.92%,  Time: 0:02:34 *
Iter:   3000,  Train Loss:  0.37,  Train Acc: 89.84%,  Val Loss:  0.43,  Val Acc: 86.89%,  Time: 0:02:39 *
Iter:   3100,  Train Loss:  0.38,  Train Acc: 86.72%,  Val Loss:  0.46,  Val Acc: 86.28%,  Time: 0:02:44
Iter:   3200,  Train Loss:  0.46,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 86.88%,  Time: 0:02:49
Iter:   3300,  Train Loss:  0.42,  Train Acc: 83.59%,  Val Loss:  0.44,  Val Acc: 86.84%,  Time: 0:02:55
Iter:   3400,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 86.65%,  Time: 0:03:00
Iter:   3500,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.44,  Val Acc: 86.72%,  Time: 0:03:05
Iter:   3600,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.44,  Val Acc: 86.97%,  Time: 0:03:10
Iter:   3700,  Train Loss:  0.26,  Train Acc: 92.19%,  Val Loss:  0.45,  Val Acc: 86.64%,  Time: 0:03:16
Iter:   3800,  Train Loss:   0.3,  Train Acc: 90.62%,  Val Loss:  0.45,  Val Acc: 86.83%,  Time: 0:03:21
Iter:   3900,  Train Loss:  0.54,  Train Acc: 85.16%,  Val Loss:  0.46,  Val Acc: 86.51%,  Time: 0:03:26
Iter:   4000,  Train Loss:  0.27,  Train Acc: 92.19%,  Val Loss:  0.44,  Val Acc: 87.13%,  Time: 0:03:31
No optimization for a long time, auto-stopping...
Test Loss:  0.39,  Test Acc: 87.99%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9212    0.8530    0.8858      1000
       realty     0.8815    0.9220    0.9013      1000
       stocks     0.8109    0.8490    0.8295      1000
    education     0.9216    0.9410    0.9312      1000
      science     0.8491    0.8270    0.8379      1000
      society     0.8781    0.8570    0.8674      1000
     politics     0.8126    0.8890    0.8491      1000
       sports     0.9333    0.9240    0.9286      1000
         game     0.9223    0.8780    0.8996      1000
entertainment     0.8819    0.8590    0.8703      1000

  avg / total     0.8812    0.8799    0.8801     10000

Confusion Matrix...
[[853  26  79   4  12   7   8   3   3   5]
 [  9 922  20   3   5   9  16   2   3  11]
 [ 45  24 849   2  28   2  42   4   1   3]
 [  2   5   6 941   5  15  14   4   2   6]
 [  0   9  42  14 827  14  41   5  32  16]
 [  4  24   8  15  13 857  46   1   3  29]
 [  3   8  29  18  13  29 889   1   4   6]
 [  2  11   1   7   8   7  11 924   5  24]
 [  2   7   8   4  50   9  12  15 878  15]
 [  6  10   5  13  13  27  15  31  21 859]]
Time usage: 0:00:00

¡¾char 3¡¿
wangjl@serverx44:~/Chinese-Text-Classification-Pytorch$ python run.py --model TextCNN --embedding random
Loading data...
180000it [00:01, 151989.96it/s]
Vocab size: 2894
180000it [00:02, 68303.24it/s]
10000it [00:00, 35145.13it/s]
10000it [00:00, 76552.78it/s]
Time usage: 0:00:04
<bound method Module.parameters of Model(
  (embedding): Embedding(2894, 300, padding_idx=2893)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc: 12.50%,  Val Loss:   2.3,  Val Acc: 17.16%,  Time: 0:00:00 *
Iter:    100,  Train Loss:   1.0,  Train Acc: 67.97%,  Val Loss:  0.69,  Val Acc: 78.54%,  Time: 0:00:04 *
Iter:    200,  Train Loss:   1.2,  Train Acc: 64.84%,  Val Loss:  0.59,  Val Acc: 81.74%,  Time: 0:00:08 *
Iter:    300,  Train Loss:   0.7,  Train Acc: 78.12%,  Val Loss:  0.53,  Val Acc: 83.63%,  Time: 0:00:12 *
Iter:    400,  Train Loss:   1.0,  Train Acc: 73.44%,  Val Loss:  0.51,  Val Acc: 84.24%,  Time: 0:00:16 *
Iter:    500,  Train Loss:  0.56,  Train Acc: 86.72%,  Val Loss:  0.48,  Val Acc: 85.46%,  Time: 0:00:20 *
Iter:    600,  Train Loss:  0.64,  Train Acc: 81.25%,  Val Loss:  0.47,  Val Acc: 85.60%,  Time: 0:00:24 *
Iter:    700,  Train Loss:  0.61,  Train Acc: 77.34%,  Val Loss:  0.46,  Val Acc: 85.93%,  Time: 0:00:28 *
Iter:    800,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 86.21%,  Time: 0:00:32 *
Iter:    900,  Train Loss:  0.63,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 86.52%,  Time: 0:00:35 *
Iter:   1000,  Train Loss:  0.45,  Train Acc: 83.59%,  Val Loss:  0.43,  Val Acc: 86.87%,  Time: 0:00:39 *
Iter:   1100,  Train Loss:  0.53,  Train Acc: 85.16%,  Val Loss:  0.42,  Val Acc: 87.17%,  Time: 0:00:43 *
Iter:   1200,  Train Loss:  0.51,  Train Acc: 85.16%,  Val Loss:  0.42,  Val Acc: 87.12%,  Time: 0:00:47 *
Iter:   1300,  Train Loss:  0.45,  Train Acc: 82.81%,  Val Loss:  0.41,  Val Acc: 87.35%,  Time: 0:00:51 *
Iter:   1400,  Train Loss:  0.61,  Train Acc: 78.12%,  Val Loss:   0.4,  Val Acc: 87.67%,  Time: 0:00:55 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.53,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 87.63%,  Time: 0:00:59
Iter:   1600,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.39,  Val Acc: 88.12%,  Time: 0:01:03 *
Iter:   1700,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 88.19%,  Time: 0:01:07 *
Iter:   1800,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 87.76%,  Time: 0:01:11
Iter:   1900,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.43%,  Time: 0:01:14 *
Iter:   2000,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:  0.39,  Val Acc: 87.98%,  Time: 0:01:18
Iter:   2100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.68%,  Time: 0:01:22 *
Iter:   2200,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 88.68%,  Time: 0:01:26 *
Iter:   2300,  Train Loss:  0.36,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 88.44%,  Time: 0:01:30
Iter:   2400,  Train Loss:  0.42,  Train Acc: 87.50%,  Val Loss:  0.38,  Val Acc: 88.58%,  Time: 0:01:34
Iter:   2500,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 88.77%,  Time: 0:01:38 *
Iter:   2600,  Train Loss:  0.41,  Train Acc: 82.81%,  Val Loss:  0.36,  Val Acc: 89.07%,  Time: 0:01:42 *
Iter:   2700,  Train Loss:  0.27,  Train Acc: 88.28%,  Val Loss:  0.37,  Val Acc: 88.35%,  Time: 0:01:46
Iter:   2800,  Train Loss:  0.54,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.86%,  Time: 0:01:49 *
Epoch [3/20]
Iter:   2900,  Train Loss:  0.45,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 89.09%,  Time: 0:01:53 *
Iter:   3000,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 89.42%,  Time: 0:01:57
Iter:   3100,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.94%,  Time: 0:02:01
Iter:   3200,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 89.00%,  Time: 0:02:05
Iter:   3300,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.09%,  Time: 0:02:09
Iter:   3400,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.36%,  Time: 0:02:13 *
Iter:   3500,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 89.31%,  Time: 0:02:17 *
Iter:   3600,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.36,  Val Acc: 89.23%,  Time: 0:02:21
Iter:   3700,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.35,  Val Acc: 89.28%,  Time: 0:02:25
Iter:   3800,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.35,  Val Acc: 89.37%,  Time: 0:02:28
Iter:   3900,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.37,  Val Acc: 88.85%,  Time: 0:02:32
Iter:   4000,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.63%,  Time: 0:02:36
Iter:   4100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.38%,  Time: 0:02:40
Iter:   4200,  Train Loss:  0.44,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 89.40%,  Time: 0:02:44
Epoch [4/20]
Iter:   4300,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.35,  Val Acc: 89.37%,  Time: 0:02:48
Iter:   4400,  Train Loss:  0.19,  Train Acc: 96.09%,  Val Loss:  0.34,  Val Acc: 89.82%,  Time: 0:02:52 *
Iter:   4500,  Train Loss:  0.31,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 89.56%,  Time: 0:02:56
Iter:   4600,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.43%,  Time: 0:02:59
Iter:   4700,  Train Loss:   0.4,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 89.30%,  Time: 0:03:03
Iter:   4800,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 89.56%,  Time: 0:03:07
Iter:   4900,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 89.56%,  Time: 0:03:11
Iter:   5000,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.69%,  Time: 0:03:15
Iter:   5100,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.52%,  Time: 0:03:19
Iter:   5200,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.35,  Val Acc: 89.93%,  Time: 0:03:23
Iter:   5300,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.71%,  Time: 0:03:27
Iter:   5400,  Train Loss:  0.52,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 89.63%,  Time: 0:03:31
No optimization for a long time, auto-stopping...
Test Loss:  0.34,  Test Acc: 89.66%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9219    0.8740    0.8973      1000
       realty     0.8986    0.9310    0.9145      1000
       stocks     0.8396    0.8320    0.8358      1000
    education     0.9329    0.9600    0.9463      1000
      science     0.8468    0.8510    0.8489      1000
      society     0.8833    0.8930    0.8881      1000
     politics     0.8857    0.8760    0.8808      1000
       sports     0.9166    0.9670    0.9411      1000
         game     0.9176    0.8910    0.9041      1000
entertainment     0.9233    0.8910    0.9069      1000

    micro avg     0.8966    0.8966    0.8966     10000
    macro avg     0.8966    0.8966    0.8964     10000
 weighted avg     0.8966    0.8966    0.8964     10000

Confusion Matrix...
[[874  18  63   6  11  10   6   8   2   2]
 [ 11 931  20   4   4  10   6   3   3   8]
 [ 44  28 832   6  37   4  37   7   4   1]
 [  0   2   2 960   6  10   5   3   3   9]
 [  4   9  30   7 851  17  19   9  41  13]
 [  3  24   5  21  10 893  24   4   1  15]
 [  7  10  28  13  18  39 876   2   1   6]
 [  0   3   1   2   3   7   5 967   2  10]
 [  2   2   6   2  53  10   5  19 891  10]
 [  3   9   4   8  12  11   6  33  23 891]]
Time usage: 0:00:00

¡¾char 4¡¿
wangjl@serverx44:~/Chinese-Text-Classification-Pytorch$ python run.py --model TextCNN
Loading data...
180000it [00:01, 150750.73it/s]
Vocab size: 2894
180000it [00:02, 69663.84it/s]
10000it [00:00, 33173.86it/s]
10000it [00:00, 54519.98it/s]
Time usage: 0:00:04
<bound method Module.parameters of Model(
  (embedding): Embedding(2894, 300, padding_idx=2893)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc: 12.50%,  Val Loss:   2.3,  Val Acc: 17.16%,  Time: 0:00:00 *
Iter:    100,  Train Loss:   1.0,  Train Acc: 67.97%,  Val Loss:  0.69,  Val Acc: 78.54%,  Time: 0:00:04 *
Iter:    200,  Train Loss:   1.2,  Train Acc: 64.84%,  Val Loss:  0.59,  Val Acc: 81.74%,  Time: 0:00:08 *
Iter:    300,  Train Loss:   0.7,  Train Acc: 78.12%,  Val Loss:  0.53,  Val Acc: 83.63%,  Time: 0:00:12 *
Iter:    400,  Train Loss:   1.0,  Train Acc: 73.44%,  Val Loss:  0.51,  Val Acc: 84.24%,  Time: 0:00:16 *
Iter:    500,  Train Loss:  0.56,  Train Acc: 86.72%,  Val Loss:  0.48,  Val Acc: 85.46%,  Time: 0:00:20 *
Iter:    600,  Train Loss:  0.64,  Train Acc: 81.25%,  Val Loss:  0.47,  Val Acc: 85.60%,  Time: 0:00:24 *
Iter:    700,  Train Loss:  0.61,  Train Acc: 77.34%,  Val Loss:  0.46,  Val Acc: 85.93%,  Time: 0:00:28 *
Iter:    800,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.44,  Val Acc: 86.21%,  Time: 0:00:32 *
Iter:    900,  Train Loss:  0.63,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 86.52%,  Time: 0:00:36 *
Iter:   1000,  Train Loss:  0.45,  Train Acc: 83.59%,  Val Loss:  0.43,  Val Acc: 86.87%,  Time: 0:00:40 *
Iter:   1100,  Train Loss:  0.53,  Train Acc: 85.16%,  Val Loss:  0.42,  Val Acc: 87.17%,  Time: 0:00:44 *
Iter:   1200,  Train Loss:  0.51,  Train Acc: 85.16%,  Val Loss:  0.42,  Val Acc: 87.12%,  Time: 0:00:48 *
Iter:   1300,  Train Loss:  0.45,  Train Acc: 82.81%,  Val Loss:  0.41,  Val Acc: 87.35%,  Time: 0:00:52 *
Iter:   1400,  Train Loss:  0.61,  Train Acc: 78.12%,  Val Loss:   0.4,  Val Acc: 87.67%,  Time: 0:00:56 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.53,  Train Acc: 82.03%,  Val Loss:  0.41,  Val Acc: 87.63%,  Time: 0:01:00
Iter:   1600,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.39,  Val Acc: 88.12%,  Time: 0:01:04 *
Iter:   1700,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 88.19%,  Time: 0:01:08 *
Iter:   1800,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 87.76%,  Time: 0:01:12
Iter:   1900,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.43%,  Time: 0:01:16 *
Iter:   2000,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:  0.39,  Val Acc: 87.98%,  Time: 0:01:20
Iter:   2100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.68%,  Time: 0:01:24 *
Iter:   2200,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.37,  Val Acc: 88.68%,  Time: 0:01:28 *
Iter:   2300,  Train Loss:  0.36,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 88.44%,  Time: 0:01:32
Iter:   2400,  Train Loss:  0.42,  Train Acc: 87.50%,  Val Loss:  0.38,  Val Acc: 88.58%,  Time: 0:01:35
Iter:   2500,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.37,  Val Acc: 88.77%,  Time: 0:01:40 *
Iter:   2600,  Train Loss:  0.41,  Train Acc: 82.81%,  Val Loss:  0.36,  Val Acc: 89.07%,  Time: 0:01:44 *
Iter:   2700,  Train Loss:  0.27,  Train Acc: 88.28%,  Val Loss:  0.37,  Val Acc: 88.35%,  Time: 0:01:47
Iter:   2800,  Train Loss:  0.54,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 88.86%,  Time: 0:01:51 *
Epoch [3/20]
Iter:   2900,  Train Loss:  0.45,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 89.09%,  Time: 0:01:55 *
Iter:   3000,  Train Loss:  0.41,  Train Acc: 88.28%,  Val Loss:  0.36,  Val Acc: 89.42%,  Time: 0:01:59
Iter:   3100,  Train Loss:  0.28,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 88.94%,  Time: 0:02:03
Iter:   3200,  Train Loss:  0.43,  Train Acc: 87.50%,  Val Loss:  0.37,  Val Acc: 89.00%,  Time: 0:02:07
Iter:   3300,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.09%,  Time: 0:02:11
Iter:   3400,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.36%,  Time: 0:02:15 *
Iter:   3500,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 89.31%,  Time: 0:02:19 *
Iter:   3600,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.36,  Val Acc: 89.23%,  Time: 0:02:23
Iter:   3700,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.35,  Val Acc: 89.28%,  Time: 0:02:27
Iter:   3800,  Train Loss:  0.43,  Train Acc: 85.16%,  Val Loss:  0.35,  Val Acc: 89.37%,  Time: 0:02:31
Iter:   3900,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.37,  Val Acc: 88.85%,  Time: 0:02:35
Iter:   4000,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.63%,  Time: 0:02:39
Iter:   4100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.38%,  Time: 0:02:43
Iter:   4200,  Train Loss:  0.44,  Train Acc: 89.06%,  Val Loss:  0.36,  Val Acc: 89.40%,  Time: 0:02:46
Epoch [4/20]
Iter:   4300,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.35,  Val Acc: 89.37%,  Time: 0:02:50
Iter:   4400,  Train Loss:  0.19,  Train Acc: 96.09%,  Val Loss:  0.34,  Val Acc: 89.82%,  Time: 0:02:54 *
Iter:   4500,  Train Loss:  0.31,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 89.56%,  Time: 0:02:58
Iter:   4600,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.36,  Val Acc: 89.43%,  Time: 0:03:02
Iter:   4700,  Train Loss:   0.4,  Train Acc: 92.19%,  Val Loss:  0.36,  Val Acc: 89.30%,  Time: 0:03:06
Iter:   4800,  Train Loss:  0.26,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 89.56%,  Time: 0:03:10
Iter:   4900,  Train Loss:  0.23,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 89.56%,  Time: 0:03:14
Iter:   5000,  Train Loss:  0.37,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.69%,  Time: 0:03:18
Iter:   5100,  Train Loss:  0.35,  Train Acc: 89.06%,  Val Loss:  0.35,  Val Acc: 89.52%,  Time: 0:03:22
Iter:   5200,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.35,  Val Acc: 89.93%,  Time: 0:03:26
Iter:   5300,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.71%,  Time: 0:03:30
Iter:   5400,  Train Loss:  0.52,  Train Acc: 85.94%,  Val Loss:  0.36,  Val Acc: 89.63%,  Time: 0:03:34
No optimization for a long time, auto-stopping...
Test Loss:  0.34,  Test Acc: 89.66%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9219    0.8740    0.8973      1000
       realty     0.8986    0.9310    0.9145      1000
       stocks     0.8396    0.8320    0.8358      1000
    education     0.9329    0.9600    0.9463      1000
      science     0.8468    0.8510    0.8489      1000
      society     0.8833    0.8930    0.8881      1000
     politics     0.8857    0.8760    0.8808      1000
       sports     0.9166    0.9670    0.9411      1000
         game     0.9176    0.8910    0.9041      1000
entertainment     0.9233    0.8910    0.9069      1000

    micro avg     0.8966    0.8966    0.8966     10000
    macro avg     0.8966    0.8966    0.8964     10000
 weighted avg     0.8966    0.8966    0.8964     10000

Confusion Matrix...
[[874  18  63   6  11  10   6   8   2   2]
 [ 11 931  20   4   4  10   6   3   3   8]
 [ 44  28 832   6  37   4  37   7   4   1]
 [  0   2   2 960   6  10   5   3   3   9]
 [  4   9  30   7 851  17  19   9  41  13]
 [  3  24   5  21  10 893  24   4   1  15]
 [  7  10  28  13  18  39 876   2   1   6]
 [  0   3   1   2   3   7   5 967   2  10]
 [  2   2   6   2  53  10   5  19 891  10]
 [  3   9   4   8  12  11   6  33  23 891]]
Time usage: 0:00:00
